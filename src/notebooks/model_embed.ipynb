{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import math\n",
    "import os\n",
    "\n",
    "from tokenizers import Tokenizer, normalizers, models, pre_tokenizers, decoders, trainers, processors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchmetrics import AUROC, Accuracy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.tuner.tuning import Tuner \n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, EarlyStopping, StochasticWeightAveraging\n",
    "from pytorch_lightning.loggers.neptune import NeptuneLogger\n",
    "import neptune.new as neptune\n",
    "\n",
    "from data_processing.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rbp24Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, longest_seq, transform=None):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.longest = longest_seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        df = self.df\n",
    "\n",
    "        seq = df['seq'][idx].lower()\n",
    "        label = df['label'][idx]\n",
    "\n",
    "        tokenized_seq = self.tokenizer.encode(seq).ids\n",
    "        padded_seq = np.pad(tokenized_seq, (0, self.longest - len(tokenized_seq)))\n",
    "\n",
    "        sample = {'seq':padded_seq, 'label':label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"Convert both seq and label to Tensors\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        seq, label = sample['seq'], sample['label']\n",
    "\n",
    "        sample = {'seq': torch.tensor(seq, dtype=torch.long),\n",
    "                  'label': torch.tensor(label)}\n",
    "        \n",
    "        return sample\n",
    "\n",
    "\n",
    "def collate_predict(batch):\n",
    "    return [item['seq'] for item in batch]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "class MultiheadAttention(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        # Note that in many implementations you see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim//2)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "\n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnCNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(AttnCNN, self).__init__() \n",
    "\n",
    "        self.save_hyperparameters(config)\n",
    "\n",
    "        self.learning_rate = self.hparams.learning_rate\n",
    "        self.decay_factor = self.hparams.decay_factor\n",
    "        self.batch_size = self.hparams.batch_size\n",
    "        \n",
    "        self.auroc = AUROC(num_classes=1)\n",
    "        self.acc = Accuracy()\n",
    "\n",
    "        # Emmbedding\n",
    "        self.embedd = nn.Embedding(self.hparams.vocab_size, self.hparams.embedding_dim, padding_idx=0)\n",
    "\n",
    "        # Convolutional blocks\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.hparams.embedding_dim, out_channels=self.hparams.CONV1_out_channels, kernel_size=self.hparams.CONV_kernelsize, padding=1),\n",
    "            nn.BatchNorm1d(self.hparams.CONV1_out_channels),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(0.3))\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.hparams.CONV1_out_channels, out_channels=self.hparams.CONV2_out_channels, kernel_size=self.hparams.CONV_kernelsize, padding=1),\n",
    "            nn.BatchNorm1d(self.hparams.CONV2_out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3))\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(5, stride=2, padding=1)\n",
    "        #self.conv3 = nn.Conv1d(in_channels=self.hparams.num_channels, out_channels=self.hparams.num_channels // 2, kernel_size=1, padding=0, bias=True)\n",
    "        \n",
    "        # Biderectional LSTM\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=self.hparams.CONV2_out_channels,\n",
    "            hidden_size=self.hparams.LSTM_num_features,\n",
    "            num_layers=1,\n",
    "            #dropout=0.25,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            proj_size=self.hparams.LSTM_proj_size)\n",
    "\n",
    "        # Multihead attention\n",
    "        self.multihead_attn = MultiheadAttention(input_dim=self.hparams.LSTM_proj_size*2, embed_dim=self.hparams.LSTM_proj_size*2, num_heads=2) \n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        if self.hparams.DIMRED:\n",
    "            self.hparams.num_channels = self.hparams.num_channels // 2\n",
    "      \n",
    "        if self.hparams.LSTM:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear((((self.hparams.seq_lenght - 3) // 2) - 2) * self.hparams.LSTM_proj_size, self.hparams.DENSE_kernelsize), #self.hparams.seq_lenght*self.hparams.LSTM_num_features*2\n",
    "                nn.BatchNorm1d(self.hparams.DENSE_kernelsize),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(self.hparams.DENSE_kernelsize, 2))\n",
    "        else:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(self.hparams.embedding_dim*2*128, self.hparams.DENSE_kernelsize),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.25),\n",
    "                nn.BatchNorm1d(self.hparams.DENSE_kernelsize),\n",
    "                nn.Linear(self.hparams.DENSE_kernelsize, 2))\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        #print(f\"Input shape: {inputs.shape}\")\n",
    "        embeds = self.embedd(inputs).permute(0,2,1) #batch, embed_dim, seq_lenght\n",
    "        #print(f\"Embedding shape: {embeds.shape}\")\n",
    "        x = self.conv1(embeds) #batch, embed_dim, seq_lenght\n",
    "        #print(f\"Conv1 output: {x.shape}\")\n",
    "        if self.hparams.POOL:\n",
    "            x = self.pool(x)\n",
    "            #print(f\"Pool output: {x.shape}\")\n",
    "        if self.hparams.CONV2:\n",
    "            x = self.conv2(x) #batch, embed_dim, seq_lenght\n",
    "            #print(f\"Conv2 output: {x.shape}\")\n",
    "        if self.hparams.LSTM:\n",
    "            x = x.permute(0,2,1) #batch, seq_lenght, embed_dim\n",
    "            #print(f\"LSTM input: {x.shape}\")\n",
    "            x,_ = self.lstm(x) #batch, seq_lenght , embed_dim\n",
    "            #print(f\"LSTM output: {x.shape}\")\n",
    "        if self.hparams.ATTN:\n",
    "            x = self.multihead_attn(x) #batch, seq_lenght , embed_dim\n",
    "            #print(f\"ATTN output: {x.shape}\")\n",
    "            x = x.permute(0,2,1) #batch, embed_dim, seq_lenght\n",
    "            #print(f\"permute output: {x.shape}\")\n",
    "        if self.hparams.DIMRED:\n",
    "            x = self.conv3(x)\n",
    "            #print(f\"DimReduction output: {x.shape}\")\n",
    "        x = self.flatten(x)\n",
    "        #print(f\"Flatten output: {x.shape}\")\n",
    "        x = self.linear(x)\n",
    "        #print(x.shape)\n",
    "  \n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = LambdaLR(optimizer, lambda epoch: self.decay_factor ** epoch)\n",
    "        return [optimizer], [scheduler] \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch['seq'], batch['label']\n",
    "        outputs = self(inputs)\n",
    "        preds = torch.max(outputs, 1)[1]\n",
    "\n",
    "        criterion = nn.NLLLoss()\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        labels = labels.cpu().detach().int()\n",
    "        preds = preds.cpu().detach().int()\n",
    "\n",
    "        train_acc = self.acc(preds, labels)\n",
    "        train_auroc = self.auroc(preds, labels)\n",
    "        \n",
    "        return {\"loss\": loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"train_auroc\": train_auroc}\n",
    "                \n",
    "\n",
    "    def training_epoch_end(self, train_step_outputs):\n",
    "        loss = torch.stack([x[\"loss\"] for x in train_step_outputs]).mean()\n",
    "        train_acc_epoch = torch.stack([x[\"train_acc\"] for x in train_step_outputs]).mean()\n",
    "        train_auroc_epoch = torch.stack([x[\"train_auroc\"] for x in train_step_outputs]).mean()\n",
    "        \n",
    "        self.log(\"train/epoch/loss\", loss)\n",
    "        self.log(\"train/epoch/acc\", train_acc_epoch)\n",
    "        self.log(\"train/epoch/auroc\", train_auroc_epoch) \n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch['seq'], batch['label']\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        criterion = nn.NLLLoss()\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        labels = labels.cpu().detach()\n",
    "        preds = torch.max(outputs, 1)[1].cpu().detach()\n",
    "\n",
    "        val_acc = self.acc(preds, labels)\n",
    "        val_auroc = self.auroc(preds, labels)\n",
    "        \n",
    "        return {\"loss\": loss,\n",
    "                \"val_acc\": val_acc,\n",
    "                \"val_auroc\": val_auroc}\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        loss = torch.stack([x[\"loss\"] for x in val_step_outputs]).mean()\n",
    "        val_acc_epoch = torch.stack([x[\"val_acc\"] for x in val_step_outputs]).mean()\n",
    "        val_auroc_epoch = torch.stack([x[\"val_auroc\"] for x in val_step_outputs]).mean()\n",
    "        \n",
    "        self.log(\"val/epoch/loss\", loss)\n",
    "        self.log(\"val/epoch/acc\", val_acc_epoch)\n",
    "        self.log(\"val/epoch/auroc\", val_auroc_epoch)\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch['seq'], batch['label']\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        criterion = nn.NLLLoss()\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        labels = labels.cpu().detach()\n",
    "        preds = torch.max(outputs, 1)[1].cpu().detach()\n",
    "\n",
    "        test_acc = self.acc(preds, labels)\n",
    "        test_auroc = self.auroc(preds, labels)\n",
    "        \n",
    "        return {\"loss\": loss,\n",
    "                \"test_acc\": test_acc,\n",
    "                \"test_auroc\": test_auroc}\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        loss = torch.stack([x[\"loss\"] for x in test_step_outputs]).mean()\n",
    "        test_acc_epoch = torch.stack([x[\"test_acc\"] for x in test_step_outputs]).mean()\n",
    "        test_auroc_epoch = torch.stack([x[\"test_auroc\"] for x in test_step_outputs]).mean()\n",
    "        \n",
    "        self.log(\"test/loss\", loss)\n",
    "        self.log(\"test/acc\", test_acc_epoch)\n",
    "        self.log(\"test/auroc\", test_auroc_epoch)\n",
    "\n",
    "        return test_auroc_epoch\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch['seq'], batch['label']\n",
    "        outputs = self(inputs)\n",
    "        labels = labels.cpu().detach()\n",
    "        preds = torch.max(outputs, 1)[1].cpu().detach()\n",
    "\n",
    "        return inputs, labels, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "192 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets(train_df, test_df, tokenizer, longest_seq, val_size):\n",
    "\n",
    "    trainset = Rbp24Dataset(train_df, tokenizer, longest_seq, transform=transforms.Compose([ToTensor()]))\n",
    "    testset = Rbp24Dataset(test_df, tokenizer, longest_seq, transform=transforms.Compose([ToTensor()]))\n",
    "\n",
    "    train_labels = [int(trainset[i]['label']) for i in range(len(trainset)-1)]\n",
    "    train_idx, val_idx= train_test_split(np.arange(len(train_labels)), test_size=val_size, shuffle=True, stratify=train_labels)\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "    return trainset, testset, train_sampler, val_sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloaders(train_df, test_df, tokenizer, longest_seq, batch_size, num_workers, val_split):\n",
    "\n",
    "    trainset, testset, train_sampler, val_sampler = make_datasets(train_df, test_df, tokenizer, longest_seq, val_split)\n",
    "\n",
    "    trainloader = DataLoader(trainset, sampler=train_sampler, batch_size=batch_size, num_workers=num_workers)\n",
    "    valloader = DataLoader(trainset, batch_size=batch_size, num_workers=num_workers, sampler=val_sampler, shuffle=False)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    return trainloader, valloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatanate(dataframes):\n",
    "    train_df = pd.concat([dataframes[0], dataframes[1]], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "    test_df = pd.concat([dataframes[2], dataframes[3]], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndataset_path = \"/home/mrkvrbl/Diplomka/Data/rbp24/processed\" #/home/mrkvrbl/Diplomka/Data/rbp31/\\nprotein = \"PARCLIP_ELAVL1A\"\\ntokenizer = Tokenizer.from_file(f\\'/home/mrkvrbl/Diplomka/Data/tokenizers/transcriptome_hg19_{PARAMS[\"vocab_size\"]}words_bpe.tokenizer.json\\')\\nPARAMS[\\'name\\'] = protein\\n\\ntrain_path = dataset_path + \"/\" + protein + \"/train/original.tsv.gz\"\\ntest_path = dataset_path + \"/\" + protein + \"/test/original.tsv.gz\"\\n\\ntrain_df = pd.read_csv(train_path, delimiter=\"\\t\", index_col=0, header=0, compression=\"gzip\")\\ntest_df = pd.read_csv(test_path, delimiter=\"\\t\", index_col=0, header=0, compression=\"gzip\")\\n\\nlongest_seq = max(max([len(tokenizer.encode(seq).ids) for seq in train_df.seq]), max([len(tokenizer.encode(seq).ids) for seq in test_df.seq]))\\nPARAMS[\\'seq_lenght\\'] = longest_seq\\n\\ntrainloader, valloader, testloader = make_dataloaders(train_df, test_df, tokenizer, longest_seq, PARAMS[\"batch_size\"], PARAMS[\"num_workers\"], PARAMS[\"val_split\"])\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "dataset_path = \"/home/mrkvrbl/Diplomka/Data/rbp24/processed\" #/home/mrkvrbl/Diplomka/Data/rbp31/\n",
    "protein = \"PARCLIP_ELAVL1A\"\n",
    "tokenizer = Tokenizer.from_file(f'/home/mrkvrbl/Diplomka/Data/tokenizers/transcriptome_hg19_{PARAMS[\"vocab_size\"]}words_bpe.tokenizer.json')\n",
    "PARAMS['name'] = protein\n",
    "\n",
    "train_path = dataset_path + \"/\" + protein + \"/train/original.tsv.gz\"\n",
    "test_path = dataset_path + \"/\" + protein + \"/test/original.tsv.gz\"\n",
    "\n",
    "train_df = pd.read_csv(train_path, delimiter=\"\\t\", index_col=0, header=0, compression=\"gzip\")\n",
    "test_df = pd.read_csv(test_path, delimiter=\"\\t\", index_col=0, header=0, compression=\"gzip\")\n",
    "\n",
    "longest_seq = max(max([len(tokenizer.encode(seq).ids) for seq in train_df.seq]), max([len(tokenizer.encode(seq).ids) for seq in test_df.seq]))\n",
    "PARAMS['seq_lenght'] = longest_seq\n",
    "\n",
    "trainloader, valloader, testloader = make_dataloaders(train_df, test_df, tokenizer, longest_seq, PARAMS[\"batch_size\"], PARAMS[\"num_workers\"], PARAMS[\"val_split\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "        \"model\": \"ECLAD\",\n",
    "        \"vocab_size\": 9,\n",
    "        \"embedding_dim\": 8,\n",
    "        \"CONV1_kernelsize\": 9,\n",
    "        \"CONV1_out_channels\": 12,\n",
    "        \"CONV2\": False,\n",
    "        \"CONV2_out_channels\": 16,\n",
    "        \"LSTM\": True,\n",
    "        \"LSTM_num_features\": 8,\n",
    "        \"ATTN\": True,\n",
    "        \"DIMRED\": False,\n",
    "        \"DENSE_kernelsize\":64,\n",
    "        \"batch_size\": 256,\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"decay_factor\": 0.95,\n",
    "        \"max_epochs\": 100,\n",
    "        \"num_workers\": 16,\n",
    "        \"val_split\": 0.1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/mrkvrbl/Diplomka/Data/rbp24/processed\"\n",
    "protein = \"ICLIP_TIA1\"\n",
    "tokenizer = Tokenizer.from_file(f'/home/mrkvrbl/Diplomka/Data/tokenizers/transcriptome_hg19_{PARAMS[\"vocab_size\"]}words_bpe.tokenizer.json')\n",
    "PARAMS['name'] = protein\n",
    "\n",
    "#rbp24\n",
    "train_df = pd.read_csv(dataset_path + \"/\" + protein +  \"/train/original.tsv.gz\", delimiter=\"\\t\", index_col=0, header=0, compression=\"gzip\")\n",
    "test_df = pd.read_csv(dataset_path + \"/\" + protein +  \"/test/original.tsv.gz\", delimiter=\"\\t\", index_col=0, header=0, compression=\"gzip\")\n",
    "\n",
    "#rbp31\n",
    "#train_df = pd.read_csv(dataset_path + \"/\" + protein +  \"/train/original.tsv.gz\", delimiter=\"\\t\", index_col=0, compression=\"gzip\")\n",
    "#test_df = pd.read_csv(dataset_path + \"/\" + protein +  \"/test/original.tsv.gz\", delimiter=\"\\t\", index_col=0, compression=\"gzip\")\n",
    "\n",
    "longest_seq = max(max([len(tokenizer.encode(seq).ids) for seq in train_df.seq]), max([len(tokenizer.encode(seq).ids) for seq in test_df.seq]))\n",
    "PARAMS['seq_lenght'] = longest_seq\n",
    "\n",
    "trainloader, valloader, testloader = make_dataloaders(train_df, test_df, tokenizer, longest_seq, PARAMS[\"batch_size\"], PARAMS[\"num_workers\"], PARAMS[\"val_split\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import gensim\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_words(sequences,kmer_len,s):\n",
    "\tout=[]\n",
    "\n",
    "\tfor i in sequences:\n",
    "\t\tkmer_list=[]\n",
    "\t\tfor j in range(0,(len(i)-kmer_len)+1,s):\n",
    "\t\t\tkmer_list.append(i[j:j+kmer_len])\n",
    "\n",
    "\t\tout.append(kmer_list)\n",
    "\n",
    "\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_index(string_data, wv):\n",
    "\tindex_data = []\n",
    "\tfor word in string_data:\n",
    "\t\tif word in wv:\n",
    "\t\t\tindex_data.append(wv.vocab[word].index)\n",
    "\treturn index_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = gen_words(train_df.seq, 4, 1)\n",
    "word2vec_model = \"./wordtovec\"\n",
    "\n",
    "model = gensim.models.Word2Vec(words, window=int(12 / 1), min_count=0,workers=multiprocessing.cpu_count())\n",
    "model.train(words,total_examples=len(words),epochs=100)\n",
    "model.save(word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = gensim.models.Word2Vec.load(word2vec_model)\n",
    "weights = torch.FloatTensor(model1.wv.vectors)\n",
    "embedding = nn.Embedding.from_pretrained(weights, freeze=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data= [convert_data_to_index(el,model.wv) for el in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 5, 6, 6, 9, 9, 5, 5, 5, 5, 5, 5, 5, 9, 9, 9, 5, 6, 5, 5, 7, 6, 9, 9,\n",
       "         7, 9, 9, 9, 5, 7, 5, 9, 7, 9, 9, 5, 6, 5, 5, 5, 5, 9, 9, 6, 5, 7, 9, 5,\n",
       "         9, 5, 7, 5, 5, 9, 5, 7, 9, 7, 7, 9, 5, 5, 5, 7, 5, 5, 9, 7, 9, 5, 7, 7,\n",
       "         6, 9, 6, 9, 7, 9, 5, 7, 6, 6, 9, 5, 6, 6, 9, 7, 6, 9, 9, 7, 5, 7, 9, 9,\n",
       "         9, 7, 5, 5, 9, 5, 6, 5, 7, 7, 6, 9, 6, 9, 6, 5, 6, 6, 9, 5, 6, 9, 5, 7,\n",
       "         6, 9, 7, 5, 7, 9, 7, 9]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = train_df.seq[1]\n",
    "tokens = torch.tensor(tokenizer.encode(seq).ids, dtype=torch.long).unsqueeze(0)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 54])\n",
      "torch.Size([1, 8, 54])\n",
      "torch.Size([1, 16, 52])\n",
      "torch.Size([1, 16, 52])\n",
      "torch.Size([1, 32, 50])\n",
      "torch.Size([1, 50, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([800])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = train_df.seq[3]\n",
    "tokens = torch.tensor(tokenizer.encode(seq).ids, dtype=torch.long).unsqueeze(0)\n",
    "print(tokens.shape)\n",
    "embed = nn.Embedding(128, 8)\n",
    "conv1 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=5, padding=1)\n",
    "pool = nn.MaxPool1d(3, stride=1, padding=1)\n",
    "conv2 = nn.Conv1d(16,32,3)\n",
    "lstm = nn.LSTM(32,32, batch_first=True, proj_size=16)\n",
    "result = embed(tokens).permute(0,2,1)\n",
    "print(result.shape)\n",
    "result = conv1(result)\n",
    "print(result.shape)\n",
    "result = pool(result)\n",
    "print(result.shape)\n",
    "result = conv2(result)\n",
    "print(result.shape)\n",
    "result = lstm(result.permute(0,2,1))\n",
    "print(result[0].shape)\n",
    "result[0].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq shape: torch.Size([256, 128])\n",
      "label shape: torch.Size([256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([15, 11, 58, 15, 15, 15, 15, 15, 15, 15, 43, 43, 43, 43, 43, 14, 32, 13,\n",
       "        19, 62, 15, 15, 55, 63, 55, 45, 18, 10, 21, 13, 45, 33,  7, 29, 15, 36,\n",
       "        15, 16, 21, 10, 12, 24, 49, 54, 22, 30,  5,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(trainloader)\n",
    "batch = next(it)\n",
    "seq, label = batch['seq'], batch['label']\n",
    "print(f\"seq shape: {seq.shape}\\nlabel shape: {label.shape}\")\n",
    "seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping('val/epoch/loss', patience=15, check_on_train_epoch_end=False, )\n",
    "\n",
    "checkpoints_path = \"checkpoints/\" + PARAMS[\"name\"]\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "        dirpath=checkpoints_path,\n",
    "        filename=\"{epoch:02d}\",\n",
    "        save_weights_only=True,\n",
    "        save_top_k=-1,\n",
    "        save_last=True,\n",
    "        monitor=\"val/epoch/loss\",\n",
    "        every_n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NeptuneLogger\n",
    "#neptune_logger = NeptuneLogger(\n",
    "#    api_key=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxZDI3YzE1Yy0yYzllLTRjM2YtYjk2MS1jNzNiZmI3MzIyNWEifQ==\",  # replace with your own\n",
    "#    project=\"mrkvrbl/MasterThesis\",  # \"<WORKSPACE/PROJECT>\"\n",
    "#    name=PARAMS[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrkvrbl/miniconda3/envs/diplomka/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:59: LightningDeprecationWarning: Setting `Trainer(stochastic_weight_avg=True)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.stochastic_weight_avg.StochasticWeightAveraging` directly to the Trainer's `callbacks` argument instead.\n",
      "  \"Setting `Trainer(stochastic_weight_avg=True)` is deprecated in v1.5 and will be removed in v1.7.\"\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/mrkvrbl/miniconda3/envs/diplomka/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(#logger=neptune_logger,\n",
    "                callbacks=[model_checkpoint, early_stopping],\n",
    "                max_epochs=PARAMS['max_epochs'],\n",
    "                accumulate_grad_batches=1,\n",
    "                gradient_clip_val=0.5,\n",
    "                stochastic_weight_avg=True,\n",
    "                gpus=1)\n",
    "model = AttnCNN(PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neptune_logger.log_model_summary(model=model, max_depth=-1)\n",
    "#neptune_logger.log_hyperparams(params=PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | auroc          | AUROC              | 0     \n",
      "1 | acc            | Accuracy           | 0     \n",
      "2 | embedd         | Embedding          | 512   \n",
      "3 | conv1          | Sequential         | 688   \n",
      "4 | conv2          | Sequential         | 2.7 K \n",
      "5 | pool           | MaxPool1d          | 0     \n",
      "6 | lstm           | LSTM               | 11.3 K\n",
      "7 | multihead_attn | MultiheadAttention | 952   \n",
      "8 | flatten        | Flatten            | 0     \n",
      "9 | linear         | Sequential         | 37.7 K\n",
      "------------------------------------------------------\n",
      "53.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "53.8 K    Total params\n",
      "0.215     Total estimated model params size (MB)\n",
      "/home/mrkvrbl/miniconda3/envs/diplomka/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/mrkvrbl/Diplomka/src/checkpoints/ALKBH5_Baltz2012 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrkvrbl/miniconda3/envs/diplomka/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:662: UserWarning: Your `val_dataloader` has `shuffle=True`, it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrkvrbl/miniconda3/envs/diplomka/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:   0%|          | 0/10 [00:00<?, ?it/s, loss=0.63, v_num=148]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrkvrbl/miniconda3/envs/diplomka/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:   0%|          | 0/10 [00:19<?, ?it/s, loss=0.63, v_num=148]"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=trainloader, val_dataloaders=valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at checkpoints/C22ORF28_Baltz2012/epoch=00-v1.ckpt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AttnCNN:\n\tsize mismatch for embedd.weight: copying a param with shape torch.Size([128, 8]) from checkpoint, the shape in current model is torch.Size([512, 8]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_327/389890783.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckpoints\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoints_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtest_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtest_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test/auroc\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_auc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule, test_dataloaders)\u001b[0m\n\u001b[1;32m    909\u001b[0m             )\n\u001b[1;32m    910\u001b[0m             \u001b[0mdataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_and_handle_interrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     def _test_impl(\n",
      "\u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \"\"\"\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[0;31m# run test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtested_ckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;31m# check if we should delay restoring checkpoint till later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_checkpoint_after_pre_dispatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_modules_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_configure_sharded_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# allow user to setup in model sharded environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_restore_modules_and_callbacks\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;31m# restore modules after setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_datamodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrainerFn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFITTING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\u001b[0m in \u001b[0;36mrestore_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# restore model state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# reset metrics states on non-rank 0 as all states have been accumulated on rank 0 via syncing on checkpointing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mload_model_state_dict\u001b[0;34m(self, checkpoint)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_optimizer_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1483\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1484\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AttnCNN:\n\tsize mismatch for embedd.weight: copying a param with shape torch.Size([128, 8]) from checkpoint, the shape in current model is torch.Size([512, 8])."
     ]
    }
   ],
   "source": [
    "checkpoints = sorted(os.listdir(checkpoints_path))\n",
    "best_auc = 0\n",
    "\n",
    "for checkpoint in checkpoints:\n",
    "    checkpoint_path = str(checkpoints_path) + \"/\" + checkpoint\n",
    "    test_out = trainer.test(model, ckpt_path=checkpoint_path, dataloaders=testloader)[0]\n",
    "\n",
    "    if test_out[\"test/auroc\"] > best_auc:\n",
    "        best_auc = test_out[\"test/auroc\"]\n",
    "        best_checkpoint_path = checkpoint_path\n",
    "print(best_auc, best_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at checkpoints/C22ORF28_Baltz2012/last.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at checkpoints/C22ORF28_Baltz2012/last.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 4/4 [00:00<00:00,  6.21it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test/acc': 0.6973329782485962,\n",
      " 'test/auroc': 0.6963504552841187,\n",
      " 'test/loss': 0.5539897084236145}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 4/4 [00:00<00:00,  5.25it/s]\n"
     ]
    }
   ],
   "source": [
    "test_out = trainer.test(model, ckpt_path=checkpoint_path, dataloaders=testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at checkpoints/C22ORF28_Baltz2012/last.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at checkpoints/C22ORF28_Baltz2012/last.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 66it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "predict_out = trainer.predict(model, ckpt_path=checkpoint_path, dataloaders=testloader, return_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [256] at entry 0 and [10] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2124/3721574159.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredict_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [256] at entry 0 and [10] at entry 1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run MST-348 received stop signal. Exiting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for the remaining 2 operations to synchronize with Neptune. Do not kill this process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 2 operations synced, thanks for waiting!\n"
     ]
    }
   ],
   "source": [
    "torch.stack([x[2] for x in predict_out]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = sorted(os.listdir(checkpoints_path))\n",
    "best_auc = 0\n",
    "\n",
    "for checkpoint in checkpoints:\n",
    "    checkpoint_path = str(checkpoints_path) + \"/\" + checkpoint\n",
    "    predict_out = trainer.predict(model, ckpt_path=checkpoint_path, dataloaders=testloader, return_predictions=True)\n",
    "\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for outs in predict_out:\n",
    "        inputs.append(outs[0])\n",
    "        labels.append(outs[1])\n",
    "        preds.append(outs[2])\n",
    "\n",
    "    inputs = [item for sublist in inputs for item in sublist]\n",
    "    labels = [item for sublist in labels for item in sublist]\n",
    "    preds = [item for sublist in preds for item in sublist]\n",
    "\n",
    "    auroc = AUROC(num_classes=1)\n",
    "    auc = auroc(torch.tensor(preds), torch.tensor(labels))\n",
    "\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "    print(auc)\n",
    "print(best_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mrkvrbl/Diplomka/best_checkpoints/CAPRIN1_Baltz2012/best.ckpt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "dst_path = \"/home/mrkvrbl/Diplomka/best_checkpoints/\" + PARAMS['name'] +\"/best.ckpt\"\n",
    "shutil.copyfile(best_checkpoint_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at checkpoints/ALKBH5_Baltz2012/epoch=43.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at checkpoints/ALKBH5_Baltz2012/epoch=43.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: 9it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#test_model = AttnCNN(PARAMS)\n",
    "#best_checkpoint = \"/home/mrkvrbl/Diplomka/src/checkpoints/ALKBH5_Baltz2012/epoch=05.ckpt\"\n",
    "#test_model.load_from_checkpoint(best_checkpoint_path)\n",
    "\n",
    "#test_trainer = Trainer()\n",
    "\n",
    "predict_out = trainer.predict(model, ckpt_path=best_checkpoint_path, dataloaders=testloader, return_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "labels = []\n",
    "preds = []\n",
    "for outs in predict_out:\n",
    "    inputs.append(outs[0])\n",
    "    labels.append(outs[1])\n",
    "    preds.append(outs[2])\n",
    "\n",
    "inputs = [item for sublist in inputs for item in sublist]\n",
    "labels = [item for sublist in labels for item in sublist]\n",
    "preds = [item for sublist in preds for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6515) AUROC()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrkvrbl/miniconda3/envs/diplomka/lib/python3.7/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "FP, FN = [], []\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "    if labels[i] < preds[i]:\n",
    "        FP.append(inputs[i])\n",
    "    elif labels[i] > preds[i]:\n",
    "        FN.append(inputs[i])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "FP = [\"\".join(tokenizer.decode(FP[i].numpy()).split(\" \")) for i in range(len(FP))]\n",
    "FN = [\"\".join(tokenizer.decode(FN[i].numpy()).split(\" \")) for i in range(len(FN))]\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "(tn, fp, fn, tp)\n",
    "\n",
    "auroc = AUROC(num_classes=1)\n",
    "auc = auroc(torch.tensor(preds), torch.tensor(labels))\n",
    "print(auc, auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        attagactataagtatcttgaagataaggtcaataaacactcatca...\n",
       "4        tgctgtgccacacaatttactgagacaatcatatcttcctaagcat...\n",
       "5        tatattgactatatctctttttattcccccttttttttttttaaag...\n",
       "6        agcacatactgttgcctttggggggaaacaaatttcctggatatgt...\n",
       "7        tattatattgtgtcttattttccctttgcaggctggtttaccatga...\n",
       "                               ...                        \n",
       "16025    aagaagaggaagaatagcaagaagaagaaccagccgggcaagtaca...\n",
       "16031    agcacatccaatggagtcagttcaacctccaaggcagaagctgtag...\n",
       "16034    tacatatatagtcactggcatactgagaatatacaatgatcctgga...\n",
       "16035    tcctagtgccaaaggttcaacttaatgtatatacctgaaaacccat...\n",
       "16037    atcttccattatcaaatgcagatacatagaaaggcagtacatcagg...\n",
       "Name: seq, Length: 8140, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save FP and FN\n",
    "train_df[train_df.label == 1].seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pos gc: 43.728573309797575\n",
      " Neg GC: 44.49588494800405\n",
      " fp_gc: 45.34202383303707\n",
      " fn_gc: 41.51364338424575\n"
     ]
    }
   ],
   "source": [
    "#pos gc: 49.06997114591921\tneg gc: 45.089285714285715\n",
    "from utils.utils import get_gc_count_from_seq\n",
    "pos_gc = get_gc_count_from_seq(train_df[train_df.label == 1].seq)\n",
    "neg_gc = get_gc_count_from_seq(train_df[train_df.label == 0].seq)\n",
    "fp_gc = get_gc_count_from_seq(FP)\n",
    "fn_gc = get_gc_count_from_seq(FN)\n",
    "\n",
    "print(f\" Pos GC: {pos_gc}\\n Neg GC: {neg_gc}\\n fp_GC: {fp_gc}\\n fn_GC: {fn_gc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import shutil\n",
    "#shutil.rmtree(checkpoints_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def baseline_model_torch_metrics(X_train, X_test, y_train, y_test, max_iter=200):\n",
    "    baseline = LogisticRegression(max_iter=max_iter, random_state=42)\n",
    "\n",
    "    # flatten the data\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "    baseline.fit(X_train_flat, y_train)\n",
    "\n",
    "    y_train = torch.tensor(y_train.values.astype(int))\n",
    "    y_test = torch.tensor(y_test.values.astype(int))\n",
    "\n",
    "\n",
    "    baseline_pred_train = torch.from_numpy(baseline.predict(X_train_flat)).int()\n",
    "    baseline_pred_test = torch.from_numpy(baseline.predict(X_test_flat)).int()\n",
    "\n",
    "    auroc = AUROC(num_classes=1)\n",
    "    acc = Accuracy()\n",
    "\n",
    "    train_acc_score = acc(y_train, baseline_pred_train)\n",
    "    test_acc_score = auroc(y_test, baseline_pred_test)\n",
    "\n",
    "    train_auc_score = acc(y_train, baseline_pred_train)\n",
    "    test_auc_score = auroc(y_test, baseline_pred_test)\n",
    "\n",
    "    print(f\"train_auc_score: {train_auc_score}\\ntest_auc_score: {test_auc_score}\\ntrain_acc_score: {train_acc_score}\\ntest_acc_score: {test_acc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10199/148463152.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Diplomka/src/utils/utils.py\u001b[0m in \u001b[0;36mget_X_y\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mDevide\u001b[0m \u001b[0mdf\u001b[0m \u001b[0mto\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply\u001b[0m \u001b[0mchanges\u001b[0m \u001b[0mto\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0musually\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \"\"\"\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_to_ohe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \"\"\"\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/diplomka/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all input arrays must have the same shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "from utils.utils import get_X_y\n",
    "\n",
    "X_train, y_train = get_X_y(train_df)\n",
    "X_test, y_test = get_X_y(test_df)\n",
    "\n",
    "result = baseline_model_torch_metrics(X_train, X_test, y_train, y_test, max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: ReadTimeout\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=00.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=01.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=02.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=03.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=04.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=05.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=06.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=07.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=08.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=09.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=10.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=11.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=12.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=13.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/last.ckpt: Path not found or is a not a file.\n",
      "Communication with Neptune restored!\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=00.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=01.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=02.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=03.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=04.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=05.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=06.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=07.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=08.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=09.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=10.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=11.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=12.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=13.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=14.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=15.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=16.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/last.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=00.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=01.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=02.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=03.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=04.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=05.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=06.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=07.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=08.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=09.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=10.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=11.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=12.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=13.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=14.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=15.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=16.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=17.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=18.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=19.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/last.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=00.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=01.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=02.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=03.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=04.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=05.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=06.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=07.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=08.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=09.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=10.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=11.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=12.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=13.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=14.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=15.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=16.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=17.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=18.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=19.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=20.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=21.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=22.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/last.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=00.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=01.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=02.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=03.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=04.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=05.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=06.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=07.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=08.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=09.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=10.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=11.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=12.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=13.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=14.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=15.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=16.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=17.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=18.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=19.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=20.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=21.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=22.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=23.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=24.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=25.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/last.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=00.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=01.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=02.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=03.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=04.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=05.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=06.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=07.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=08.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=09.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=10.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=11.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=12.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=13.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=14.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=15.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=16.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=17.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=18.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=19.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=20.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=21.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=22.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=23.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=24.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=25.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=26.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=27.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=28.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/epoch=29.ckpt: Path not found or is a not a file.\n",
      "Error occurred during asynchronous operation processing: Cannot upload file /home/mrkvrbl/Diplomka/src/checkpoints/PARCLIP_MOV10_Sievers/last.ckpt: Path not found or is a not a file.\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterConnectionError\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterConnectionError\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterConnectionError\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterConnectionError\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterConnectionError\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterConnectionError\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterConnectionError\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterConnectionError\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterConnectionError\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterConnectionError\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterConnectionError\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Communication with Neptune restored!\n"
     ]
    }
   ],
   "source": [
    "# ADD EMBEDING AND CONFUSION TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a28f15e7b4b39adb7072b19439694c39c45d283f7f075ccd030e70179c18cdfd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('diplomka')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
